{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02167e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: [[3.9234231e-04 9.9952352e-01 8.4139654e-05]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"Car.jpg\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a dog\", \"a Car\", \"a cat\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ade70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1720128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to C:\\Users\\H504171/.cache\\cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 169001437/169001437 [00:30<00:00, 5471480.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\H504171/.cache\\cifar-100-python.tar.gz to C:\\Users\\H504171/.cache\n",
      "\n",
      "Top predictions:\n",
      "\n",
      "           snake: 65.31%\n",
      "          turtle: 12.29%\n",
      "    sweet_pepper: 3.83%\n",
      "          lizard: 1.88%\n",
      "       crocodile: 1.75%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "# Download the dataset\n",
    "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
    "\n",
    "# Prepare the inputs\n",
    "image, class_id = cifar100[3637]\n",
    "image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
    "\n",
    "# Calculate features\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "# Pick the top 5 most similar labels for the image\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "values, indices = similarity[0].topk(5)\n",
    "\n",
    "# Print the result\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3e473b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwC/aeOrOKzH2rzROFztA6++e1Yl3408QT4uLZ4LS2dl2BwGIG7Bz+Fc3Faie5ihgnXzHuHEwfOI8HKqCPX+dRKsv2WC6ubTdE0hVZQwdowAex78nrxxWPM7aMTWp6J4d+JiyTpa61AbaV8ski/c2Z4LHtXptjeRXUSTQSrJGwyro2QR9a+dY/s9nawvqGnXqx3ClopCivkdO/tzXo/wznu1e+icutmdj28UhUMgOeqjpnrirjJ3syHHqeZ3elXFtqwuCfJSKRpy5bAkONwXj6Yz71U09pvtEFrcylFiiaaV3YfKW5UgH72BtyBXbX9nI6yq+W3psVeD8p9a5/UPDtyIdiwNLE0gkMuMMhAIC88Y5/SuWFTozrqUnuiNJb+fRb28aWO5jt1URozDChuCwX247d61fBGq3Om6hDdvtMU4EM+3hRtb5ep68npWbEJ01ARiXzY40do42tQgyRxkKOeceoosbeNJ7Q3aiN0yC+0AlieM/wCPvWjmlqZKEpaH/9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e98950e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch3.8\\lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a white car parked on the side of the road . \n"
     ]
    }
   ],
   "source": [
    "import open_clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "model, _, transform = open_clip.create_model_and_transforms(\n",
    "  model_name=\"coca_ViT-L-14\",\n",
    "  pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\"\n",
    ")\n",
    "\n",
    "im = Image.open(\"Car.jpg\").convert(\"RGB\")\n",
    "im = transform(im).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    generated = model.generate(im)\n",
    "\n",
    "print(open_clip.decode(generated[0]).split(\"<end_of_text>\")[0].replace(\"<start_of_text>\", \"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3.8",
   "language": "python",
   "name": "pytorch3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
