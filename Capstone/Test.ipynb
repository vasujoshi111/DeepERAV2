{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a8f855-e8e6-4718-8766-83102d4bf4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d59e144a-62c7-4fee-aa6c-ac9d5bb39a7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python get_coco.py --coco_path './' --data_dir './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f990c32-c25c-4e44-b3a4-bff6a2933ad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a65eb8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\.conda\\envs\\torch_vasu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Admin\\.conda\\envs\\torch_vasu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:557: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 4 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 30.60s/it]\n",
      "c:\\Users\\Admin\\.conda\\envs\\torch_vasu\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint weights for projection layer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dataset import get_data_loaders\n",
    "from transformers import AutoTokenizer\n",
    "from model import CustomClipPhi2, train_model\n",
    "from configs import get_config\n",
    "\n",
    "config = get_config() \n",
    "# tokenizer\n",
    "tokenizer  = AutoTokenizer.from_pretrained(config.get(\"phi2_model_name\"), trust_remote_code=True)\n",
    "\n",
    "# data loaders\n",
    "train_dataloader, val_dataloader = get_data_loaders(config.get(\"data_dir\"), config.get(\"clip_model_name\"), tokenizer, config.get(\"train_batch_size\"), config.get(\"val_batch_size\"), config.get(\"num_workers\"))\n",
    "\n",
    "llmModel = CustomClipPhi2(tokenizer, config.get(\"phi2_model_name\"), config.get(\"clip_model_name\"), clip_embed=768, phi_embed=2560).to(config.get(\"device\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0d6bb7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomClipPhi2(\n",
       "  (phi2_model): PhiForCausalLM(\n",
       "    (model): PhiModel(\n",
       "      (embed_tokens): Embedding(51200, 2560)\n",
       "      (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x PhiDecoderLayer(\n",
       "          (self_attn): PhiAttention(\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rotary_emb): PhiRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): PhiMLP(\n",
       "            (activation_fn): NewGELUActivation()\n",
       "            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
       "  )\n",
       "  (clip_model): CLIPVisionModel(\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "        (position_embedding): Embedding(197, 768)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (projection_layer): Linear(in_features=768, out_features=2560, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmModel.projection_layer.load_state_dict(torch.load(\"./ckpts/model.pth\", map_location=config.get(\"device\")))\n",
    "llmModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08254b85-114d-42c0-b31e-70863155bb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Target captions: A black Honda motorcycle parked in front of a garage. \n",
      " ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ \n",
      " Predicted_captions:A motorcycle motorcycle parked in a a a a..........<|endoftext|> \n",
      "0 - Target captions: A black Honda motorcycle parked in front of a garage. \n",
      " ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ \n",
      " Predicted_captions:A motorcycle motorcycle parked in a a a a..........<|endoftext|> \n"
     ]
    }
   ],
   "source": [
    "from model import show_results_for_samples\n",
    "show_results_for_samples(llmModel, val_dataloader, tokenizer, config, num_samples = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26457e53-2888-40fb-8c7a-0ca353e675ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d15b06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
